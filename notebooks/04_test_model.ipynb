{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giIsKmQcBFNy"
      },
      "source": [
        "# Fake News Detector - Test Model\n",
        "The notebook covers the testing the model workflow run on ISOT Fake News detection dataset, provided by Kaggle.\n",
        "\n",
        "The Kaggle Link : https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHRQLsnsA-oB",
        "outputId": "adcfa47e-5c03-4d49-a806-adac3c66b4e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install kagglehub[hf-datasets] pandas --quiet\n",
        "!python -m nltk.downloader punkt_tab wordnet stopwords > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvBZsBVEBRwq"
      },
      "outputs": [],
      "source": [
        "# Step 2: Import libraries\n",
        "import re\n",
        "from time import time\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBDDqQoB13e"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oYFDVXLCiCc",
        "outputId": "f202c9aa-4f92-4235-db4d-28210638ed2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully from /content/fake_news_model.pkl\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model_path = \"/content/fake_news_model.pkl\"\n",
        "\n",
        "try:\n",
        "    model = joblib.load(model_path)\n",
        "    print(f\"Model loaded successfully from {model_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Model file not found at {model_path}\")\n",
        "    model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5NOXF58DyLV"
      },
      "source": [
        "## Defining the text preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PX4OnGaPDyWl"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters/numbers except basic punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Remove short words (length < 2)\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nihoK5HwCMEi"
      },
      "source": [
        "## Build test samples function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZdmyA_NB323",
        "outputId": "dc1dcabb-90d8-44e5-c109-9129d78b849d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ§ª Sample Predictions:\n",
            "\n",
            "Text: Scientists confirm climate change is accelerating at unprecedented rates. New da...\n",
            "True: Real\n",
            "Pred: Fake\n",
            "Confidence: 64.0%\n",
            "Probabilities: [Real: 0.3603, Fake: 0.6397]\n",
            "\n",
            "Text: BREAKING: Celebrities injecting themselves with alien DNA to gain immortality, s...\n",
            "True: Fake\n",
            "Pred: Fake\n",
            "Confidence: 91.2%\n",
            "Probabilities: [Real: 0.0882, Fake: 0.9118]\n",
            "\n",
            "Text: New study finds that regular exercise can reduce the risk of heart disease by up...\n",
            "True: Real\n",
            "Pred: Fake\n",
            "Confidence: 59.6%\n",
            "Probabilities: [Real: 0.4036, Fake: 0.5964]\n",
            "\n",
            "Text: Government secretly adding mind-control chemicals to drinking water supplies nat...\n",
            "True: Fake\n",
            "Pred: Fake\n",
            "Confidence: 54.4%\n",
            "Probabilities: [Real: 0.4562, Fake: 0.5438]\n"
          ]
        }
      ],
      "source": [
        "# Test with sample predictions\n",
        "def test_samples(model):\n",
        "    samples = [\n",
        "        (\"Scientists confirm climate change is accelerating at unprecedented rates. New data shows ice caps melting 40% faster than previous estimates.\", 0),\n",
        "        (\"BREAKING: Celebrities injecting themselves with alien DNA to gain immortality, secret documents reveal!\", 1),\n",
        "        (\"New study finds that regular exercise can reduce the risk of heart disease by up to 30%\", 0),\n",
        "        (\"Government secretly adding mind-control chemicals to drinking water supplies nationwide\", 1)\n",
        "    ]\n",
        "\n",
        "    print(\"\\nðŸ§ª Sample Predictions:\")\n",
        "    for text, true_label in samples:\n",
        "        processed = preprocess_text(text)\n",
        "        prediction = model.predict([processed])[0]\n",
        "        proba = model.predict_proba([processed])[0]\n",
        "\n",
        "        print(f\"\\nText: {text[:80]}...\")\n",
        "        print(f\"True: {'Fake' if true_label == 1 else 'Real'}\")\n",
        "        print(f\"Pred: {'Fake' if prediction == 1 else 'Real'}\")\n",
        "        print(f\"Confidence: {max(proba)*100:.1f}%\")\n",
        "        print(f\"Probabilities: [Real: {proba[0]:.4f}, Fake: {proba[1]:.4f}]\")\n",
        "\n",
        "# Test samples\n",
        "test_samples(model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
