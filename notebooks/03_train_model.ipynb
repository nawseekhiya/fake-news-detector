{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fake News Detector - Train Model\n",
        "The notebook covers the training the model workflow run on ISOT Fake News detection dataset, provided by Kaggle.\n",
        "\n",
        "The Kaggle Link : https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset"
      ],
      "metadata": {
        "id": "BY4Txb_sssi7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cirQVXsIsfT1",
        "outputId": "ee311c16-c7be-476a-b37e-7642ccf17eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install kagglehub[hf-datasets] pandas --quiet\n",
        "!python -m nltk.downloader punkt_tab wordnet stopwords > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import libraries\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import joblib\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from datasets import Dataset\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "Qwvwmc1atGHO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset from Kaggle"
      ],
      "metadata": {
        "id": "lvO2LIYwtH9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets directly from Kaggle\n",
        "def load_dataset():\n",
        "    print(\"‚è≥ Loading datasets from Kaggle...\")\n",
        "    fake_ds = kagglehub.load_dataset(\n",
        "        KaggleDatasetAdapter.HUGGING_FACE,\n",
        "        \"clmentbisaillon/fake-and-real-news-dataset\",\n",
        "        \"Fake.csv\",\n",
        "        hf_kwargs={\"split\": \"all\"}\n",
        "    )\n",
        "\n",
        "    true_ds = kagglehub.load_dataset(\n",
        "        KaggleDatasetAdapter.HUGGING_FACE,\n",
        "        \"clmentbisaillon/fake-and-real-news-dataset\",\n",
        "        \"True.csv\",\n",
        "        hf_kwargs={\"split\": \"all\"}\n",
        "    )\n",
        "\n",
        "    # Convert to pandas DataFrames\n",
        "    fake_df = fake_ds.to_pandas()\n",
        "    fake_df['label'] = 1  # Fake news\n",
        "\n",
        "    true_df = true_ds.to_pandas()\n",
        "    true_df['label'] = 0  # Real news\n",
        "\n",
        "    # Combine datasets\n",
        "    df = pd.concat([fake_df, true_df], ignore_index=True)\n",
        "\n",
        "    print(f\"‚úÖ Dataset loaded: {len(df)} records\")\n",
        "    print(f\"   - Fake news: {len(fake_df)} samples\")\n",
        "    print(f\"   - Real news: {len(true_df)} samples\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "full_df = load_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDEakVmWulp5",
        "outputId": "f54cd300-9444-41d9-f365-cb0cf8b43622"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Loading datasets from Kaggle...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-492e9c0ad2ae>:4: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  fake_ds = kagglehub.load_dataset(\n",
            "<ipython-input-3-492e9c0ad2ae>:11: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  true_ds = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded: 44898 records\n",
            "   - Fake news: 23481 samples\n",
            "   - Real news: 21417 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data"
      ],
      "metadata": {
        "id": "xHiScx_cvRQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define project structure paths\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
        "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "cHt5SsZ9vWiu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters/numbers except basic punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Remove short words (length < 2)\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "1Cr2mMzIvZF-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full preprocessing workflow\n",
        "def preprocess_data(df):\n",
        "    print(\"\\nüßπ Preprocessing data...\")\n",
        "    start = time()\n",
        "\n",
        "    # Save raw data\n",
        "    raw_path = DATA_RAW / \"raw_combined.csv\"\n",
        "    df.to_csv(raw_path, index=False)\n",
        "    print(f\"‚úÖ Raw data saved to: {raw_path}\")\n",
        "\n",
        "    # Clean data\n",
        "    df = df.drop_duplicates(subset=['title', 'text'])\n",
        "    df['text'] = df['text'].fillna('')\n",
        "    df['title'] = df['title'].fillna('')\n",
        "\n",
        "    # Combine title and text\n",
        "    df['full_text'] = df['title'] + ' ' + df['text']\n",
        "\n",
        "    # Preprocess text in batches\n",
        "    batch_size = 2000\n",
        "    batches = [df.iloc[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
        "\n",
        "    processed_texts = []\n",
        "    for i, batch in enumerate(batches):\n",
        "        print(f\"  Processing batch {i+1}/{len(batches)}\")\n",
        "        processed_batch = batch['full_text'].apply(preprocess_text)\n",
        "        processed_texts.extend(processed_batch)\n",
        "\n",
        "    df['processed_text'] = processed_texts\n",
        "\n",
        "    # Create train-test split\n",
        "    train_df = df.sample(frac=0.8, random_state=42)\n",
        "    test_df = df.drop(train_df.index)\n",
        "\n",
        "    # Save processed data\n",
        "    train_path = DATA_PROCESSED / \"train_processed.csv\"\n",
        "    test_path = DATA_PROCESSED / \"test_processed.csv\"\n",
        "\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Preprocessing completed in {time()-start:.2f} seconds\")\n",
        "    print(f\"   - Train set ({len(train_df)} samples): {train_path}\")\n",
        "    print(f\"   - Test set ({len(test_df)} samples): {test_path}\")\n",
        "\n",
        "    # Save additional artifacts for future reference\n",
        "    sample_path = DATA_PROCESSED / \"sample_processed_texts.csv\"\n",
        "    train_df[['processed_text', 'label']].head(100).to_csv(sample_path, index=False)\n",
        "    print(f\"   - Sample processed texts: {sample_path}\")\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Preprocess the data\n",
        "train_df, test_df = preprocess_data(full_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzE8nz89vqNK",
        "outputId": "68aca177-7357-444c-e7f6-4f1a0c56f7cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßπ Preprocessing data...\n",
            "‚úÖ Raw data saved to: /content/data/raw/raw_combined.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-735a2a0b9af1>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['text'].fillna('')\n",
            "<ipython-input-6-735a2a0b9af1>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['title'] = df['title'].fillna('')\n",
            "<ipython-input-6-735a2a0b9af1>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['full_text'] = df['title'] + ' ' + df['text']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing batch 1/20\n",
            "  Processing batch 2/20\n",
            "  Processing batch 3/20\n",
            "  Processing batch 4/20\n",
            "  Processing batch 5/20\n",
            "  Processing batch 6/20\n",
            "  Processing batch 7/20\n",
            "  Processing batch 8/20\n",
            "  Processing batch 9/20\n",
            "  Processing batch 10/20\n",
            "  Processing batch 11/20\n",
            "  Processing batch 12/20\n",
            "  Processing batch 13/20\n",
            "  Processing batch 14/20\n",
            "  Processing batch 15/20\n",
            "  Processing batch 16/20\n",
            "  Processing batch 17/20\n",
            "  Processing batch 18/20\n",
            "  Processing batch 19/20\n",
            "  Processing batch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-735a2a0b9af1>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['processed_text'] = processed_texts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Preprocessing completed in 207.38 seconds\n",
            "   - Train set (31284 samples): /content/data/processed/train_processed.csv\n",
            "   - Test set (7821 samples): /content/data/processed/test_processed.csv\n",
            "   - Sample processed texts: /content/data/processed/sample_processed_texts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "NvSBgx8zvVKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train_model(X_train, y_train):\n",
        "    print(\"\\nü§ñ Training model...\")\n",
        "    start = time()\n",
        "\n",
        "    # Create pipeline\n",
        "    model = make_pipeline(\n",
        "    TfidfVectorizer(max_features=10000, ngram_range=(1, 2)),\n",
        "    LogisticRegression(\n",
        "        C=0.5,\n",
        "        max_iter=100,\n",
        "        random_state=42\n",
        "    )\n",
        ")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_time = time() - start\n",
        "    print(f\"‚úÖ Training completed in {train_time:.2f} seconds\")\n",
        "    return model, train_time\n",
        "\n",
        "# Prepare data\n",
        "X_train = train_df['processed_text']\n",
        "y_train = train_df['label']\n",
        "X_test = test_df['processed_text']\n",
        "y_test = test_df['label']\n",
        "\n",
        "# Train the model\n",
        "model, train_time = train_model(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0Z6gmfEv1rz",
        "outputId": "2f48927a-c4ec-4011-892d-61c5c01cc30a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ü§ñ Training model...\n",
            "‚úÖ Training completed in 35.32 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model"
      ],
      "metadata": {
        "id": "zbHhJTuMv6aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "def save_artifacts(model):\n",
        "    print(\"\\nüíæ Saving artifacts...\")\n",
        "    # Save model\n",
        "    joblib.dump(model, 'fake_news_model.pkl')\n",
        "\n",
        "    # Save sample data for testing\n",
        "    test_df[['processed_text', 'label']].to_csv('test_samples.csv', index=False)\n",
        "\n",
        "    # Download files\n",
        "    from google.colab import files\n",
        "    files.download('fake_news_model.pkl')\n",
        "    files.download('test_samples.csv')\n",
        "\n",
        "    print(\"‚úÖ Model and artifacts saved. Files downloaded to your computer.\")\n",
        "\n",
        "# Save everything\n",
        "save_artifacts(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "Hola6j6jv8Ka",
        "outputId": "ad25a704-bd5f-48e8-b2ec-e5f7a533ee0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Saving artifacts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_625d4e9b-a63d-4e44-8ff5-1c45d3ab0698\", \"fake_news_model.pkl\", 469124)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_97be6f6f-7012-4182-8999-a74bcb34f4c2\", \"test_samples.csv\", 13770733)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model and artifacts saved. Files downloaded to your computer.\n"
          ]
        }
      ]
    }
  ]
}