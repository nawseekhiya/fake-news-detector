{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fake News Detector - Data Preprocessing\n",
        "The notebook covers the Data Preprocessing workflow run on ISOT Fake News detection dataset, provided by Kaggle.\n",
        "\n",
        "The Kaggle Link : https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset"
      ],
      "metadata": {
        "id": "DWk_4RIO05cX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJUHTdJp0s_v",
        "outputId": "82268293-5bed-461a-fc35-4c55e7851b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install kagglehub[hf-datasets] pandas --quiet\n",
        "!python -m nltk.downloader punkt_tab wordnet stopwords > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter"
      ],
      "metadata": {
        "id": "NrkR2evb1rBK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data from Kaggle"
      ],
      "metadata": {
        "id": "ESReCuYl2B-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    print(\"‚è≥ Loading datasets from Kaggle...\")\n",
        "\n",
        "    fake_ds = kagglehub.load_dataset(\n",
        "        KaggleDatasetAdapter.HUGGING_FACE,\n",
        "        \"clmentbisaillon/fake-and-real-news-dataset\",\n",
        "        \"Fake.csv\",\n",
        "        hf_kwargs={\"split\": \"all\"}\n",
        "    )\n",
        "    true_ds = kagglehub.load_dataset(\n",
        "        KaggleDatasetAdapter.HUGGING_FACE,\n",
        "        \"clmentbisaillon/fake-and-real-news-dataset\",\n",
        "        \"True.csv\",\n",
        "        hf_kwargs={\"split\": \"all\"}\n",
        "    )\n",
        "\n",
        "    fake_df = fake_ds.to_pandas()\n",
        "    true_df = true_ds.to_pandas()\n",
        "\n",
        "    fake_df['label'] = 1  # Fake news\n",
        "    true_df['label'] = 0  # Real news\n",
        "\n",
        "    df = pd.concat([fake_df, true_df], ignore_index=True)\n",
        "\n",
        "    print(f\"‚úÖ Dataset loaded: {len(df)} records\")\n",
        "    print(f\"   - Fake news: {len(fake_df)} samples\")\n",
        "    print(f\"   - Real news: {len(true_df)} samples\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "5RmOajrR2HI5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define project structure paths\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
        "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "j0pk7oEI2eaM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process individual text entries"
      ],
      "metadata": {
        "id": "U5Kqlfsu2o4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters/numbers except basic punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Remove short words (length < 2)\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "Zp5xwKIV2vTw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    print(\"\\nüßπ Preprocessing data...\")\n",
        "    start = time()\n",
        "\n",
        "    # Save raw data\n",
        "    raw_path = DATA_RAW / \"raw_combined.csv\"\n",
        "    df.to_csv(raw_path, index=False)\n",
        "    print(f\"‚úÖ Raw data saved to: {raw_path}\")\n",
        "\n",
        "    # Clean data\n",
        "    df = df.drop_duplicates(subset=['title', 'text'])\n",
        "    df['text'] = df['text'].fillna('')\n",
        "    df['title'] = df['title'].fillna('')\n",
        "\n",
        "    # Combine title and text\n",
        "    df['full_text'] = df['title'] + ' ' + df['text']"
      ],
      "metadata": {
        "id": "ucZsW1mP3Oqb"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}