{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fake News Detector - Data Preprocessing\n",
        "The notebook covers the Data Preprocessing workflow run on ISOT Fake News detection dataset, provided by Kaggle.\n",
        "\n",
        "The Kaggle Link : https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset"
      ],
      "metadata": {
        "id": "DWk_4RIO05cX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJUHTdJp0s_v",
        "outputId": "a0424d91-da0b-4163-ac9f-f66051de5e45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install kagglehub[hf-datasets] pandas --quiet\n",
        "!python -m nltk.downloader punkt_tab wordnet stopwords > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter"
      ],
      "metadata": {
        "id": "NrkR2evb1rBK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data from Kaggle"
      ],
      "metadata": {
        "id": "ESReCuYl2B-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    print(\"‚è≥ Loading datasets from Kaggle...\")\n",
        "\n",
        "    fake_ds = kagglehub.load_dataset(\n",
        "        KaggleDatasetAdapter.HUGGING_FACE,\n",
        "        \"clmentbisaillon/fake-and-real-news-dataset\",\n",
        "        \"Fake.csv\",\n",
        "        hf_kwargs={\"split\": \"all\"}\n",
        "    )\n",
        "    true_ds = kagglehub.load_dataset(\n",
        "        KaggleDatasetAdapter.HUGGING_FACE,\n",
        "        \"clmentbisaillon/fake-and-real-news-dataset\",\n",
        "        \"True.csv\",\n",
        "        hf_kwargs={\"split\": \"all\"}\n",
        "    )\n",
        "\n",
        "    fake_df = fake_ds.to_pandas()\n",
        "    true_df = true_ds.to_pandas()\n",
        "\n",
        "    fake_df['label'] = 1  # Fake news\n",
        "    true_df['label'] = 0  # Real news\n",
        "\n",
        "    df = pd.concat([fake_df, true_df], ignore_index=True)\n",
        "\n",
        "    print(f\"‚úÖ Dataset loaded: {len(df)} records\")\n",
        "    print(f\"   - Fake news: {len(fake_df)} samples\")\n",
        "    print(f\"   - Real news: {len(true_df)} samples\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "full_df = load_dataset()"
      ],
      "metadata": {
        "id": "5RmOajrR2HI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045938e9-14df-4899-efd9-d148005476e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Loading datasets from Kaggle...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-f6275523bdb9>:4: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  fake_ds = kagglehub.load_dataset(\n",
            "<ipython-input-15-f6275523bdb9>:10: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  true_ds = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded: 44898 records\n",
            "   - Fake news: 23481 samples\n",
            "   - Real news: 21417 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define project structure paths\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
        "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "j0pk7oEI2eaM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process individual text entries"
      ],
      "metadata": {
        "id": "U5Kqlfsu2o4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters/numbers except basic punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Remove short words (length < 2)\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "Zp5xwKIV2vTw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full preprocess data workflow"
      ],
      "metadata": {
        "id": "9cZpRzOtbhGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    print(\"\\nüßπ Preprocessing data...\")\n",
        "    start = time()\n",
        "\n",
        "    # Save raw data\n",
        "    raw_path = DATA_RAW / \"raw_combined.csv\"\n",
        "    df.to_csv(raw_path, index=False)\n",
        "    print(f\"‚úÖ Raw data saved to: {raw_path}\")\n",
        "\n",
        "    # Clean data\n",
        "    df = df.drop_duplicates(subset=['title', 'text'])\n",
        "    df['text'] = df['text'].fillna('')\n",
        "    df['title'] = df['title'].fillna('')\n",
        "\n",
        "    # Combine title and text\n",
        "    df['full_text'] = df['title'] + ' ' + df['text']\n",
        "\n",
        "    # Preprocess text in batches\n",
        "    batch_size = 2000\n",
        "    batches = [df.iloc[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
        "\n",
        "    processed_texts = []\n",
        "    for i, batch in enumerate(batches):\n",
        "        print(f\"  Processing batch {i+1}/{len(batches)}\")\n",
        "        processed_batch = batch['full_text'].apply(preprocess_text)\n",
        "        processed_texts.extend(processed_batch)\n",
        "\n",
        "    df['processed_text'] = processed_texts\n",
        "\n",
        "    # Create train-test split\n",
        "    train_df = df.sample(frac=0.8, random_state=42)\n",
        "    test_df = df.drop(train_df.index)\n",
        "\n",
        "    # Save processed data\n",
        "    train_path = DATA_PROCESSED / \"train_processed.csv\"\n",
        "    test_path = DATA_PROCESSED / \"test_processed.csv\"\n",
        "\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Preprocessing completed in {time()-start:.2f} seconds\")\n",
        "    print(f\"   - Train set ({len(train_df)} samples): {train_path}\")\n",
        "    print(f\"   - Test set ({len(test_df)} samples): {test_path}\")\n",
        "\n",
        "    # Save additional artifacts for future reference\n",
        "    sample_path = DATA_PROCESSED / \"sample_processed_texts.csv\"\n",
        "    train_df[['processed_text', 'label']].head(100).to_csv(sample_path, index=False)\n",
        "    print(f\"   - Sample processed texts: {sample_path}\")\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Preprocess the data\n",
        "train_df, test_df = preprocess_data(full_df)"
      ],
      "metadata": {
        "id": "ucZsW1mP3Oqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817f0d66-00d5-4980-9b85-8569ab071028"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßπ Preprocessing data...\n",
            "‚úÖ Raw data saved to: /content/data/raw/raw_combined.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-a4e32100acbc>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['text'].fillna('')\n",
            "<ipython-input-18-a4e32100acbc>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['title'] = df['title'].fillna('')\n",
            "<ipython-input-18-a4e32100acbc>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['full_text'] = df['title'] + ' ' + df['text']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing batch 1/20\n",
            "  Processing batch 2/20\n",
            "  Processing batch 3/20\n",
            "  Processing batch 4/20\n",
            "  Processing batch 5/20\n",
            "  Processing batch 6/20\n",
            "  Processing batch 7/20\n",
            "  Processing batch 8/20\n",
            "  Processing batch 9/20\n",
            "  Processing batch 10/20\n",
            "  Processing batch 11/20\n",
            "  Processing batch 12/20\n",
            "  Processing batch 13/20\n",
            "  Processing batch 14/20\n",
            "  Processing batch 15/20\n",
            "  Processing batch 16/20\n",
            "  Processing batch 17/20\n",
            "  Processing batch 18/20\n",
            "  Processing batch 19/20\n",
            "  Processing batch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-a4e32100acbc>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['processed_text'] = processed_texts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Preprocessing completed in 209.12 seconds\n",
            "   - Train set (31284 samples): /content/data/processed/train_processed.csv\n",
            "   - Test set (7821 samples): /content/data/processed/test_processed.csv\n",
            "   - Sample processed texts: /content/data/processed/sample_processed_texts.csv\n"
          ]
        }
      ]
    }
  ]
}